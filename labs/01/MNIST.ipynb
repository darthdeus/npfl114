{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import datetime\n",
    "import tensorflow.contrib.summary as tfsum\n",
    "from tensorflow.examples.tutorials import mnist\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting .\\train-images-idx3-ubyte.gz\n",
      "Extracting .\\train-labels-idx1-ubyte.gz\n",
      "Extracting .\\t10k-images-idx3-ubyte.gz\n",
      "Extracting .\\t10k-labels-idx1-ubyte.gz\n",
      "(5, 5, 1, 32)\n",
      "INFO:tensorflow:Summary name conv2d/kernel:0/weights is illegal; using conv2d/kernel_0/weights instead.\n",
      "(3, 3, 32, 32)\n",
      "INFO:tensorflow:Summary name conv2d_1/kernel:0/weights is illegal; using conv2d_1/kernel_0/weights instead.\n",
      "(5, 5, 32, 64)\n",
      "INFO:tensorflow:Summary name conv2d_2/kernel:0/weights is illegal; using conv2d_2/kernel_0/weights instead.\n",
      "(3, 3, 64, 64)\n",
      "INFO:tensorflow:Summary name conv2d_3/kernel:0/weights is illegal; using conv2d_3/kernel_0/weights instead.\n",
      "(5, 5, 64, 128)\n",
      "INFO:tensorflow:Summary name conv2d_4/kernel:0/weights is illegal; using conv2d_4/kernel_0/weights instead.\n",
      "(3, 3, 128, 128)\n",
      "INFO:tensorflow:Summary name conv2d_5/kernel:0/weights is illegal; using conv2d_5/kernel_0/weights instead.\n"
     ]
    }
   ],
   "source": [
    "dataset = mnist.input_data.read_data_sets(\".\", reshape=False, seed=42)\n",
    "\n",
    "tf.get_default_graph().as_default()\n",
    "tf.reset_default_graph()\n",
    "\n",
    "images = tf.placeholder(tf.float32, [None, 28, 28, 1], name=\"images\")\n",
    "labels = tf.placeholder(tf.int64, [None], name=\"labels\")\n",
    "\n",
    "\n",
    "UNITS = 784\n",
    "LAYERS = 0\n",
    "ACTIVATION = \"relu\"\n",
    "\n",
    "activations = {\n",
    "    \"none\": None,\n",
    "    \"relu\": tf.nn.relu,\n",
    "    \"tanh\": tf.nn.tanh,\n",
    "    \"sigmoid\": tf.nn.sigmoid,\n",
    "}\n",
    "\n",
    "training_flag = tf.placeholder_with_default(False, shape=(), name=\"training_flag\")\n",
    "\n",
    "hidden = images\n",
    "\n",
    "def conv_block(inputs, filters, kernel_size = (3,3), pool = False):\n",
    "    conv = tf.layers.conv2d(inputs, filters, kernel_size, padding=\"SAME\", activation=tf.nn.relu)\n",
    "    res = tf.layers.batch_normalization(conv, training=training_flag)\n",
    "    \n",
    "    if pool:\n",
    "        res = tf.layers.max_pooling2d(res, (2,2), (2,2), padding=\"SAME\") \n",
    "    \n",
    "    return res\n",
    "\n",
    "hidden = conv_block(hidden, 32, (5,5))\n",
    "hidden = conv_block(hidden, 32, (3,3), pool=True)\n",
    "\n",
    "hidden = conv_block(hidden, 64, (5,5))\n",
    "hidden = conv_block(hidden, 64, (3,3), pool=True)\n",
    "\n",
    "hidden = conv_block(hidden, 128, (5,5))\n",
    "hidden = conv_block(hidden, 128, (3,3), pool=True)\n",
    "\n",
    "\n",
    "hidden = tf.layers.flatten(hidden, name=\"flatten\")\n",
    "\n",
    "hidden = tf.layers.dense(hidden, units=UNITS, name=\"hidden_a\", activation=activations[ACTIVATION])\n",
    "hidden = tf.layers.batch_normalization(hidden, training=training_flag)\n",
    "hidden = tf.layers.dense(hidden, units=UNITS, name=\"hidden_b\", activation=activations[ACTIVATION])\n",
    "\n",
    "output = tf.layers.dense(hidden, 10, activation=None, name=\"output_layer\")\n",
    "predictions = tf.argmax(output, axis=1, name=\"prediction\")\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(labels, predictions), tf.float32))\n",
    "loss = tf.losses.sparse_softmax_cross_entropy(labels, output, scope=\"loss\")\n",
    "\n",
    "step = tf.train.create_global_step()\n",
    "\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    train = tf.train.AdamOptimizer().minimize(loss, global_step=step, name=\"training\")\n",
    "\n",
    "logdir = \"logs/{}-{}-{}-{}\".format(\n",
    "        UNITS,\n",
    "        LAYERS,\n",
    "        ACTIVATION,\n",
    "        datetime.datetime.now().strftime(\"%Y-%m-%d_%H%M%S\"))\n",
    "\n",
    "\n",
    "summary_writer = tfsum.create_file_writer(logdir, flush_millis=3000)\n",
    "\n",
    "def layer_histogram(layer):\n",
    "    with tf.variable_scope(layer, reuse=True):\n",
    "        return tfsum.histogram(\"weights/{}\".format(layer), tf.get_variable(\"kernel\"))\n",
    "\n",
    "summaries = {}       \n",
    "with summary_writer.as_default(), tfsum.record_summaries_every_n_global_steps(30):\n",
    "    weight_summaries = [\n",
    "        layer_histogram(\"hidden_a\"),\n",
    "        layer_histogram(\"hidden_b\"),\n",
    "        layer_histogram(\"output_layer\"),\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    for var in tf.trainable_variables():\n",
    "        if var.name.startswith(\"conv2d\") and var.name.endswith(\"kernel:0\"):\n",
    "            print(var.shape)\n",
    "            w,h,_,_ = var.shape        \n",
    "            img = tf.reshape(var[:,:,0,0], (1,w,h,1))\n",
    "            weight_summaries.append(tfsum.image(var.name + \"/weights\", img))\n",
    "        \n",
    "#     g = tf.get_default_graph()\n",
    "#     scope = conv.name.split(\"/\")[0]\n",
    "#     with tf.variable_scope(scope, reuse=True):\n",
    "#         tfsum.image(\"{}/filters\".format(scope), tf.get_variable(\"kernel:0\"))\n",
    "    \n",
    "\n",
    "    summaries[\"train\"] = [tfsum.scalar(\"train/loss\", loss),\n",
    "                          tfsum.scalar(\"train/accuracy\", accuracy),\n",
    "                          *weight_summaries]\n",
    "        \n",
    "with summary_writer.as_default(), tfsum.always_record_summaries():\n",
    "    for run in [\"dev\", \"test\"]:\n",
    "        summaries[run] = [tfsum.scalar(run + \"/accuracy\", accuracy)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [01:09<00:00,  6.97s/it]\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "session = tf.Session()\n",
    "\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "with summary_writer.as_default():\n",
    "    tf.contrib.summary.initialize(session=session, graph=tf.get_default_graph())\n",
    "\n",
    "for i in tqdm(range(EPOCHS)):\n",
    "    while dataset.train.epochs_completed == i:\n",
    "        X, y = dataset.train.next_batch(BATCH_SIZE)\n",
    "        session.run([train, summaries[\"train\"]], {images: X, labels: y, training_flag: True})\n",
    "    session.run(summaries[\"dev\"], {images: dataset.validation.images, labels: dataset.validation.labels, training_flag: False})\n",
    "    session.run(summaries[\"test\"], {images: dataset.test.images, labels: dataset.test.labels, training_flag: False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten = tf.layers.conv2d(tf.placeholder(tf.float32, shape=[None, 24, 24, 3]), 1, (3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten.name.split(\"/\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'conv2d_3/Conv2D:0' shape=(?, 22, 22, 1) dtype=float32>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ten.op.inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.trainable_variables()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'strided_slice_1:0' shape=(5, 5, 1) dtype=float32>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:,:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable conv2d/kernel:0 does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=tf.AUTO_REUSE in VarScope?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-ff9737d80809>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"conv2d\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"kernel:0\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\dev\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[0;32m   1260\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1261\u001b[0m       \u001b[0muse_resource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1262\u001b[1;33m       constraint=constraint)\n\u001b[0m\u001b[0;32m   1263\u001b[0m get_variable_or_local_docstring = (\n\u001b[0;32m   1264\u001b[0m     \"\"\"%s\n",
      "\u001b[1;32mC:\\dev\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[0;32m   1095\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1096\u001b[0m           \u001b[0muse_resource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1097\u001b[1;33m           constraint=constraint)\n\u001b[0m\u001b[0;32m   1098\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1099\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[1;32mC:\\dev\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[0;32m    433\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m           \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_resource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m           constraint=constraint)\n\u001b[0m\u001b[0;32m    436\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[1;32mC:\\dev\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[1;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[0;32m    402\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 404\u001b[1;33m           use_resource=use_resource, constraint=constraint)\n\u001b[0m\u001b[0;32m    405\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\dev\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[0;32m    759\u001b[0m       raise ValueError(\"Variable %s does not exist, or was not created with \"\n\u001b[0;32m    760\u001b[0m                        \u001b[1;34m\"tf.get_variable(). Did you mean to set \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 761\u001b[1;33m                        \"reuse=tf.AUTO_REUSE in VarScope?\" % name)\n\u001b[0m\u001b[0;32m    762\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_fully_defined\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0minitializing_from_value\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    763\u001b[0m       raise ValueError(\"Shape of a new variable (%s) must be fully defined, \"\n",
      "\u001b[1;31mValueError\u001b[0m: Variable conv2d/kernel:0 does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=tf.AUTO_REUSE in VarScope?"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"conv2d\", reuse=True): tf.get_variable(\"kernel:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
